{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**COMPUTER VISION**\n",
        "\n",
        "The purpose of this exercise is to delve deeply into convolutional neural networks and gain a comprehensive grasp of computer vision."
      ],
      "metadata": {
        "id": "8G6Qzg0modcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "cf1bf5cf-35ca-4141-c98a-0450b970ac30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 20 20:51:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "0f8e3b48-ac28-4dde-bb0c-c5f42086c7b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ],
      "metadata": {
        "id": "FSFX7tc1w-en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "computer vision can be use in different aspect of our life which include the following:\n",
        "\n",
        "**Medical image analysis:** Computer vision algorithms can analyze medical scans like X-rays, CT scans, and MRIs to assist doctors in diagnosis and treatment planning. This can lead to faster and more accurate diagnoses, as well as personalized treatment options.\n",
        "\n",
        "\n",
        "**Quality controls**\n",
        "\n",
        "**Robot guidance:** Vision systems help robots navigate and manipulate objects efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "uHsgseuNqU7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find."
      ],
      "metadata": {
        "id": "oBK-WI6YxDYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, overfitting occurs when a model memorizes the training data too closely, hindering its ability to generalize and perform well on unseen data. It's like studying only past exam questions without understanding core concepts, leading to poor performance on new questions."
      ],
      "metadata": {
        "id": "jyLgGRgVr-Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each.\n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ],
      "metadata": {
        "id": "XeYFEqw8xK26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are some of the ways that we can you to prevent overfitting in machine learning.\n",
        "\n",
        "\n",
        "***Data Augmentation:*** This technique artificially expands the training dataset by transforming existing data through various methods like cropping, rotating, or flipping images. This exposes the model to diverse variations, preventing it from overfitting to specific details in the original data. Think of it like studying with a broader range of practice problems to solidify your understanding, not just memorizing a single set.\n",
        "\n",
        "***Regularization Techniques:*** These methods penalize models for having overly complex structures, encouraging them to learn simpler representations of the data. This can involve penalizing the weights of connections between neurons in the model, preventing them from becoming too specific to individual training points. Imagine adding a penalty for memorizing individual words compared to grasping the overarching concepts in a study topic.\n",
        "\n",
        "***Early Stopping:*** This strategy involves monitoring the model's performance on a separate validation dataset while it's being trained. Once the model's performance on the validation data starts to degrade, indicating overfitting, training is stopped to prevent further memorization of the training data. It's like recognizing when you're just rote memorizing without actually learning and taking a break to solidify your understanding before pushing further."
      ],
      "metadata": {
        "id": "kcu1B5DksuVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ],
      "metadata": {
        "id": "DKdEEFEqxM-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let import the touch libraries\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the MNIST train dataset\n",
        "train_data = datasets.MNIST(root=\".\",\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transforms.ToTensor())\n",
        "# Get the MNIST test dataset\n",
        "test_data = datasets.MNIST(root=\".\",\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NScT9v3wCAU",
        "outputId": "04850101-4fe3-47c2-a11b-3058b161432e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 109686291.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 41742141.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 33897606.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 19031497.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let print the train test dataset\n",
        "train_data, test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxnTWHjrwUq6",
        "outputId": "62cee982-42c0-40c2-e04b-b6a53d1f3862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: .\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " Dataset MNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: .\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(5):\n",
        "  img = train_data[i][0]\n",
        "  print(img.shape)\n",
        "  img_squeeze = img.squeeze()\n",
        "  print(img_squeeze.shape)\n",
        "  label = train_data[i][1]\n",
        "  plt.figure(figsize=(3, 3))\n",
        "  plt.imshow(img_squeeze, cmap=\"gray\")\n",
        "  plt.title(label)\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef86e4a4-ce9f-46ba-ac33-e560f96a4e7d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let create a dataset loader\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=32,\n",
        "                             shuffle=False)\n",
        ""
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmDfXRK4xgR4",
        "outputId": "095c58c0-4cd0-4a0e-fbe3-dd3cdfb1092b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e730681e7d0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e730681f790>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let check the sample and print out the sample shape image\n",
        "for sample in next(iter(train_dataloader)):\n",
        "  print(sample.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5UCtu1vxoD1",
        "outputId": "56e00c88-eac5-493a-fae6-4e79e0e8d1d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let create the model for MNIST dataset\n",
        "from torch import nn\n",
        "class MNIST_model(torch.nn.Module):\n",
        "  \"\"\"Model capable of predicting on MNIST dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(in_features=hidden_units*7*7,\n",
        "                out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "\n",
        "    x = self.conv_block_2(x)\n",
        "\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "uJeRo9yIznIx",
        "outputId": "7de53d35-66dc-47e3-f5e5-75548f556058"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "me_FmaSDyhi2",
        "outputId": "1f14d9e9-a741-4080-d373-1b0eea153bcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNIST_model(input_shape=1,\n",
        "                    hidden_units=10,\n",
        "                    output_shape=10).to(device)\n",
        "model\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeVnFg9RzH8-",
        "outputId": "b011d894-80be-46df-d272-c953cef47297"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNIST_model(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.\n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been.\n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error?\n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}